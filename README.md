Existing speech emotion recognition systems lack accuracy and struggle to adapt to diverse speakers. They often ignore factors like age and gender, which influence emotions, and don’t clearly present performance metrics.
This project aims to develop a system that detects emotions, age, and gender from speech, enhancing accuracy with diverse datasets and visualizing results through bar graphs or pie charts for better understanding.
The proposed SER system successfully integrates feature fusion (tonal, pitch, and spectral features) and deep learning models (CNN, LSTM) to achieve higher accuracy and better generalization across diverse speakers (accents, genders, age groups). 
The system provides detailed visualizations (bar graphs, pie charts) for emotion analysis, enhancing usability and interactivity.
It addresses key limitations of existing systems, such as bias and adaptability, making it suitable for real-world applications.
